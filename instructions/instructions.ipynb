{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970e53dd-3a0c-4ab4-8137-0fe21090b985",
   "metadata": {},
   "source": [
    "# 7 Fine-tuning to follow instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f6e11c-668a-4304-b2b4-561d3a40358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.0.2\n",
      "matplotlib version: 3.10.1\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0\n",
      "tqdm version: 4.67.1\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b527c-9cd3-4ace-9e12-aa692f94db48",
   "metadata": {},
   "source": [
    "## 7.2 Preparing a dataset for supervised instruction finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cc31974-bc54-41f4-b6ed-6091cf6882d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4629a541-6f66-43b0-8c92-18b5b8bf5ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c8d0532-7d3f-459e-af12-147642e64727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"What is an antonym of 'complicated'?\",\n",
       " 'input': '',\n",
       " 'output': \"An antonym of 'complicated' is 'simple'.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2a9a050-3081-4cec-9e79-2ce73cad0037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bb75fcb-71d0-42ed-8ed6-cf8f34336f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n"
     ]
    }
   ],
   "source": [
    "print(format_input(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea20dd13-d4f3-4990-8b5c-e40aa07e6e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4f88813-c7d8-4dca-ba7d-8f55f04fdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97ef6a19-a4b9-4bb8-bff3-2bc1406ddc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602d005-ca1b-4056-a56c-877d056382e7",
   "metadata": {},
   "source": [
    "## 7.3 Organizing data into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f636fde-89d1-4688-b26a-86a655c9bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a43a490-8550-4452-8882-283fd3189440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2edf5fa1-4d47-4f93-9e2f-cf7b8f32221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7915390-a1ac-4615-b0e7-0fb3e96da3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0d9be6f-9610-4536-a665-b5cda41b2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cefdac6c-427c-449c-b238-d8a027a20f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f74ef29-6917-4f6c-81ac-80e6b7b4233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d49cb3c-8ab2-4c25-886b-5ae8657c5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3037171-c4fd-4238-927b-a54131d26f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee51ea24-4efa-4d53-b07b-c5381f0e584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n",
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)\n",
    "\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "# the -100 disallows the training mechanism from being influenced by repeated\n",
    "# padding tokens\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c87d31-5dd6-41e0-ae91-3a99b4f71107",
   "metadata": {},
   "source": [
    "## 7.4 Creating data loaders for an instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99c80ad4-3cb6-40f6-8040-dc7e92081c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7abb98c6-23f3-4db4-a52b-dd256b7428b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10de229a-98a4-45fa-88cf-a566f573cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54b5ce4d-0b31-49aa-9d18-dca0905d2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    # only difference with train_loader is no shuffle or drop last entry\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b4ebab8-4104-4ca5-86f7-d56e06f53e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f483ec76-1b64-43c7-ba55-8f6a97cd6c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# inputs contain the end of text padding tokens (50256) \n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2d4bc3c-6903-4f61-8fe6-552b3b4e3fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# targets contain the -100 placeholder tokens\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7cf83-5ea3-43f3-9b45-dc193ed60b19",
   "metadata": {},
   "source": [
    "## 7.5 Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59630e9e-6508-4b71-9e55-90f91ca379d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45c432e5-cb75-4c07-a4a4-ff9dec9206f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9323ba0c-7b48-4853-a78a-5fe92432ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4519dec-dfaf-4a98-9454-c95dc7f43ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):] # truncate input text from generated response\n",
    "    .replace(\"### Response:\", \"\") \n",
    "    .strip() # strips all unnecessary newline characters\n",
    ")\n",
    "print(response_text)\n",
    "\n",
    "# since the model is not trained yet, it cannot handle instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6275e-3308-44e6-b895-577bdf892433",
   "metadata": {},
   "source": [
    "## 7.6 Finetuning the LLM on instruction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d270b989-48e8-42b0-a537-8093d41ea266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fb68543-bcf0-4031-88c0-6262db4f7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8259105682373047\n",
      "Validation loss: 3.7619349479675295\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73c38a7e-b8dd-486a-917b-15f1b634a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
      "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.800, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.809\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
      "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.535, Val loss 0.732\n",
      "Ep 1 (Step 000075): Train loss 0.567, Val loss 0.736\n",
      "Ep 1 (Step 000080): Train loss 0.602, Val loss 0.731\n",
      "Ep 1 (Step 000085): Train loss 0.513, Val loss 0.715\n",
      "Ep 1 (Step 000090): Train loss 0.571, Val loss 0.696\n",
      "Ep 1 (Step 000095): Train loss 0.504, Val loss 0.687\n",
      "Ep 1 (Step 000100): Train loss 0.507, Val loss 0.682\n",
      "Ep 1 (Step 000105): Train loss 0.568, Val loss 0.674\n",
      "Ep 1 (Step 000110): Train loss 0.562, Val loss 0.669\n",
      "Ep 1 (Step 000115): Train loss 0.519, Val loss 0.665\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.437, Val loss 0.670\n",
      "Ep 2 (Step 000125): Train loss 0.454, Val loss 0.686\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.681\n",
      "Ep 2 (Step 000135): Train loss 0.406, Val loss 0.677\n",
      "Ep 2 (Step 000140): Train loss 0.407, Val loss 0.676\n",
      "Ep 2 (Step 000145): Train loss 0.373, Val loss 0.677\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.674\n",
      "Ep 2 (Step 000155): Train loss 0.419, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.414, Val loss 0.686\n",
      "Ep 2 (Step 000165): Train loss 0.380, Val loss 0.688\n",
      "Ep 2 (Step 000170): Train loss 0.327, Val loss 0.679\n",
      "Ep 2 (Step 000175): Train loss 0.338, Val loss 0.668\n",
      "Ep 2 (Step 000180): Train loss 0.390, Val loss 0.657\n",
      "Ep 2 (Step 000185): Train loss 0.417, Val loss 0.659\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.650\n",
      "Ep 2 (Step 000195): Train loss 0.326, Val loss 0.636\n",
      "Ep 2 (Step 000200): Train loss 0.311, Val loss 0.632\n",
      "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.628\n",
      "Ep 2 (Step 000210): Train loss 0.367, Val loss 0.628\n",
      "Ep 2 (Step 000215): Train loss 0.393, Val loss 0.635\n",
      "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.647\n",
      "Ep 2 (Step 000225): Train loss 0.346, Val loss 0.662\n",
      "Ep 2 (Step 000230): Train loss 0.298, Val loss 0.656\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked everyday by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 3.89 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dc3b809-5051-4656-bba6-93c6d2dbd363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a282e-0002-4c3b-a0a6-8e17da36f139",
   "metadata": {},
   "source": [
    "## 7.7 Extracting and saving responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79e2a031-8761-4c1b-9668-299a1bad9d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b4c82cc-14e4-4e25-8f6d-856f8f24eeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 110/110 [01:48<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "664859f9-18c8-49ab-b4e0-eaf3ca0c36a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as a bullet.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff61dd54-5ae2-4ae5-8dc1-161fca4aa3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b04bda14-dd42-4d33-9cc3-f0fe176eaaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n",
    "\n",
    "# SFT = Supervised finetuning\n",
    "# Load model via\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab2c1d-1deb-454b-b78e-8a730df1325f",
   "metadata": {},
   "source": [
    "## 7.8 Evaluating the finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47f3cf-5faf-4661-9a62-f4733960c567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

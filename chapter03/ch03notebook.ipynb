{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8e7943-d31a-47c2-bee4-f47deaac2617",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e73f2-a0f2-49d7-91cf-322282b959bd",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c6569-95ff-4007-8149-ed928684d4b9",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87064227-2fb1-4e9f-bffb-b16ceaaf088b",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b255a66-55e1-4f8d-8b08-bf45adc8c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# for simplicity, each word = one token\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77b7588-4f14-49e6-9178-33809a76cca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1]\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f494a7d-4135-463e-84ac-656a30aee79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1831e69-be29-4d2f-b3b9-2062e1252643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual visualization of dot product between [2] and [1]\n",
    "\n",
    "0.55*0.43 + 0.87*0.15 + 0.66 * 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa2ccff-73c8-4907-b2c5-47d08765c772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5b244e-2296-4233-a416-52c7056a0f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.dot(inputs[1], input_query)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1d1977-3ec2-45af-bcdf-fcada2a74f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604bf1ab-597b-488f-afad-038f593c7d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2bc060-c752-4905-96b0-81af1075bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, input_query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "# attention/similarity score is presented thru dot product of vectors in respect to x^2\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc7dbd7-9855-4668-986f-60ec221236c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# introductory method for normalization\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "attn_weights_2_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf60a86b-3a10-490e-bd6b-c73e1236a1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2_tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f556be3f-1efe-46cd-9a89-29ac757c9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns normalized tensor with optimized values for training\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ad48b6-189a-400e-a45a-0b15bc9ecfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is basically what happens in softmax function\n",
    "\n",
    "def softmax_beta(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "softmax_beta(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397665c-0589-495c-a7e0-4d52145fd1ae",
   "metadata": {},
   "source": [
    "steps 1 (input vector calculation) and 2 (attention score normalization) are complete\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a6811a-cc51-41c1-914a-97658ad935c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2053af3-3415-46ce-a47d-91040aa934e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13854756951332092 ---> tensor([0.4300, 0.1500, 0.8900])\n",
      "0.2378913015127182 ---> tensor([0.5500, 0.8700, 0.6600])\n",
      "0.23327402770519257 ---> tensor([0.5700, 0.8500, 0.6400])\n",
      "0.12399158626794815 ---> tensor([0.2200, 0.5800, 0.3300])\n",
      "0.10818186402320862 ---> tensor([0.7700, 0.2500, 0.1000])\n",
      "0.15811361372470856 ---> tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query for demo\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(f\"{attn_weights_2[i]} ---> {inputs[i]}\")\n",
    "    context_vec_2 += attn_weights_2[i] * inputs[i]\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c59c15-4237-4e40-8099-9b6fb43dd0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    print(i, inputs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6745270-a11f-419b-a26d-810c7d5db434",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "266afcae-154e-4054-9aa2-b78a77d31cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# a more generalized version, however slower due to nested for loops\n",
    "\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        # 6-6 matrix attention score\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j) \n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f24665-51b4-42aa-a1d3-9f3e5c8f02a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more optimized version using matrix multiplication\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06eb3e1d-d038-4f83-91bd-99b63472b607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizes with respect to 1\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc0d5f37-ade2-4c4d-8684-0cec7adb3d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d878f5f-992e-4680-8523-13ca8138b8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automated version of all context vectors rather than only inputs[1]\n",
    "\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f90ebcf8-644e-4067-b62c-6bf974ecee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29821dae-bd0b-4675-8048-bdbeb518a5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of updated code\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e6493-9c10-4e7e-90cd-afc2dbccb7c6",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf18db8-a1b3-405f-88f2-fc3a0965ebc6",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing the attention weights step by step\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3ea5fd4-8eaa-4402-9c70-991e1be00161",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd9f38af-4b70-4472-a588-a7e64e775a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75195bb2-836a-4c82-a28c-82066c125c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforms 3-dimensional tensor into 2-dimensional\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "009d1832-a926-43ee-ac00-2c792b1000c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "value = inputs @ W_value\n",
    "\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f82a63b8-9c35-4fbf-83ca-e2158122c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = torch.dot(query_2, keys_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cef1e2e-3889-430c-bda9-8f678fc2270d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "643ffb86-db06-4d2f-93aa-2b4c5bb18e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3. obtain attention weights by normalizing scores\n",
    "\n",
    "d_k = keys.shape[1]\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45adcd62-cfff-4c2e-87bf-44edfe2d484d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c4c9e-9182-4ecc-88c6-9cbeb80a8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4. obtain context vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
